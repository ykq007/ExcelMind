"""æµå¼å¯¹è¯ - ä½¿ç”¨æ‰‹åŠ¨å·¥å…·è§£æé¿å… LangChain å·¥å…·ç»‘å®šå…¼å®¹æ€§é—®é¢˜"""

import json
import re
from typing import Any, AsyncGenerator, Dict, Optional

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

from .domain.ports import LLMFactory, ToolRegistry
from .excel_loader import get_loader
from .knowledge_base import get_knowledge_base, format_knowledge_context
from .language import detect_target_language, is_language_mismatch, language_label, localize, rewrite_system_prompt
from .tools import ALL_TOOLS
from .utils import json_dumps


# æ„å»ºå·¥å…·æè¿°
TOOLS_DESCRIPTION = """
## Available Tools

You can use the following tools to analyze Excel data. When you need to use a tool, use this JSON format:

```json
{"tool": "tool_name", "args": {"param_name": "param_value"}}
```

### Tool List:

1. **filter_data** - Filter data by conditions (supports sorting, column selection)
   - filters (list): Multi-condition filter list, each contains column, operator, value
   - select_columns (list): Specify returned column names (optional)
   - sort_by (string): Sort column name (optional), can complete filter+sort in one step
   - ascending (bool): Sort direction, true=ascending/false=descending, default true
   - column (string): Single condition filter column name (optional)
   - operator (string): Comparison operator (==, !=, >, <, >=, <=, contains, startswith, endswith)
   - value (any type): Comparison value, supports strings, numbers, dates, etc.
   - limit (int): Return quantity limit, default 20
   - **Tip**: When filtering + sorting is needed, use this tool to complete in one step

2. **aggregate_data** - Aggregate statistics on columns (supports post-filter aggregation)
   - column (string): ã€Requiredã€‘Column name to aggregate
   - agg_func (string): ã€Requiredã€‘Aggregation function: sum, mean, count, min, max, median, std
   - filters (list): Optional filter conditions, filter first then aggregate

3. **group_and_aggregate** - Group by columns and aggregate (supports filtering)
   - group_by (string): Group column name
   - agg_column (string): Column name to aggregate
   - agg_func (string): Aggregation function (sum, mean, count, min, max)
   - filters (list): Filter conditions. **ã€Importantã€‘If user specifies date, region, etc., must pass here, otherwise will aggregate entire table**
   - limit (int): Return quantity limit, default 20

4. **search_data** - Search keywords in specified or all columns
   - keyword (string): Search keyword
   - columns (list): Limit search column names (optional)
   - select_columns (list): Specify returned column names
   - limit (int): Return quantity limit, default 20

5. **get_column_stats** - Get detailed column statistics (supports filtering)
   - column (string): Column name
   - filters (list): Optional filter conditions

6. **get_unique_values** - Get list of unique values in column (supports filtering)
   - column (string): Column name
   - filters (list): Optional filter conditions
   - limit (int): Return quantity limit, default 50

7. **get_data_preview** - Get data preview
   - n_rows (int): Preview row count, default 10

8. **get_current_time** - Get current system time
   - No parameters

9. **calculate** - Execute mathematical calculations (supports batch)
    - expressions (list): String format math expression list, e.g. ["(A+B)/C", "100*0.5"]

10. **generate_chart** - Generate ECharts visualization charts
    - chart_type (string): Chart type: bar, line, pie, scatter, radar, funnel, or "auto" for auto-recommendation
    - x_column (string): X-axis data column (required for bar/line charts)
    - y_column (string): Y-axis data column (numeric column)
    - group_by (string): Group column (required for pie/funnel charts)
    - agg_func (string): Aggregation function: sum, mean, count, min, max
    - title (string): Chart title
    - filters (list): Filter conditions
    - series_columns (list): Multi-series Y-axis column names (radar charts need at least 3)
    - limit (int): Data point quantity limit, default 20
    - **Use cases**: When users want to visualize data, generate charts, plot trends, show proportions

## Important Rules
- If you need to call a tool, only output one JSON object, no other text
- After tool call I will tell you the result, then you answer the user's question based on results
- If no tool is needed, answer directly in natural language
"""


SYSTEM_PROMPT_WITH_TOOLS = """You are a professional Excel data analysis assistant.

**ğŸŒ LANGUAGE RULE (HIGHEST PRIORITY)**
TARGET RESPONSE LANGUAGE: {target_language}

You MUST respond in {target_language}, even if the spreadsheet/knowledge/tool outputs contain other languages.
You MAY quote column names or cell values in their original language, but the surrounding explanation must be in {target_language}.

## Current Excel Information
{excel_summary}

## Related Knowledge Reference
{knowledge_context}

{tools_description}

## Working Principles
1. **CRITICAL: The Excel data is ALREADY LOADED and accessible via tools. NEVER ask the user to provide data, column sums, or any information that can be obtained through tools.**
2. **When the user asks for data across multiple columns (e.g., "monthly totals", "all months"), you MUST call the tool multiple times (once per column) to gather ALL the required data before responding.**
3. Based on user questions, determine if tools are needed
4. If tools needed, **immediately call the appropriate tool** and **only output** tool call JSON, **strictly prohibit** any other text, thinking process, or explanation
5. After successful tool call, if you need more data from other columns, **immediately call the tool again** for the next column
6. After gathering ALL required data, answer user questions based on complete results
7. **In final answer, directly provide conclusions and analysis**, do not describe "I used xx tool" or "I performed xx operation" or other internal processes
8. **ALWAYS use tools to retrieve data instead of asking the user for it**
9. Maintain friendly tone and provide data analysis recommendations
10. If there is related knowledge reference, follow the rules and suggestions within
"""





def get_llm(llm_factory: Optional[LLMFactory] = None):
    """è·å– LLM å®ä¾‹ï¼ˆæ”¯æŒ DI æ³¨å…¥ï¼‰"""
    if llm_factory is not None:
        return llm_factory.create_chat_model()
    # å‘åå…¼å®¹ï¼šä½¿ç”¨å®¹å™¨
    from .core import get_container
    return get_container().get_llm_factory().create_chat_model()


def parse_tool_call(text: str) -> Dict[str, Any] | None:
    """ä»æ–‡æœ¬ä¸­è§£æå·¥å…·è°ƒç”¨ JSONï¼ˆæ”¯æŒåµŒå¥—ç»“æ„ï¼‰"""
    # å°è¯•åŒ¹é… JSON ä»£ç å—
    json_match = re.search(r'```json\s*(\{.*?\})\s*```', text, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(1))
        except json.JSONDecodeError:
            pass
    
    # å°è¯•æå–å®Œæ•´çš„ JSON å¯¹è±¡ï¼ˆæ”¯æŒåµŒå¥—ï¼‰
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ…å« "tool" çš„ { å¼€å§‹ï¼Œç„¶ååŒ¹é…æ‹¬å·
    start_idx = text.find('{')
    while start_idx != -1:
        # å°è¯•ä»è¿™ä¸ªä½ç½®æå–å®Œæ•´JSON
        depth = 0
        end_idx = start_idx
        in_string = False
        escape_next = False
        
        for i, char in enumerate(text[start_idx:], start_idx):
            if escape_next:
                escape_next = False
                continue
            if char == '\\' and in_string:
                escape_next = True
                continue
            if char == '"' and not escape_next:
                in_string = not in_string
                continue
            if in_string:
                continue
            if char == '{':
                depth += 1
            elif char == '}':
                depth -= 1
                if depth == 0:
                    end_idx = i + 1
                    break
        
        if depth == 0 and end_idx > start_idx:
            candidate = text[start_idx:end_idx]
            try:
                parsed = json.loads(candidate)
                if isinstance(parsed, dict) and "tool" in parsed:
                    return parsed
            except json.JSONDecodeError:
                pass
        
        # ç»§ç»­æ‰¾ä¸‹ä¸€ä¸ª {
        start_idx = text.find('{', start_idx + 1)
    
    return None


def execute_tool(tool_name: str, tool_args: dict, tool_registry: Optional[ToolRegistry] = None) -> dict:
    """æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆæ”¯æŒ DI æ³¨å…¥ï¼‰"""
    if tool_registry is not None:
        return tool_registry.execute(tool_name, tool_args)
    # å‘åå…¼å®¹ï¼šä½¿ç”¨å…¨å±€å·¥å…·åˆ—è¡¨
    for tool in ALL_TOOLS:
        if tool.name == tool_name:
            try:
                return tool.invoke(tool_args)
            except Exception as e:
                return {"error": str(e)}
    return {"error": f"Tool not found: {tool_name}"}


async def stream_chat(message: str, history: list = None) -> AsyncGenerator[Dict[str, Any], None]:
    """æ‰§è¡Œå¯¹è¯
    
    Args:
        message: å½“å‰ç”¨æˆ·æ¶ˆæ¯
        history: å†å²å¯¹è¯åˆ—è¡¨ï¼Œæ¯é¡¹ä¸º {"role": "user"|"assistant", "content": "..."}
    """
    loader = get_loader()
    target_language = detect_target_language(message)

    if not loader.is_loaded:
        yield {
            "type": "error",
            "content": localize(
                target_language,
                en="Please upload an Excel file first.",
                zh="è¯·å…ˆä¸Šä¼  Excel æ–‡ä»¶",
            ),
        }
        return

    try:
        excel_summary = loader.get_summary(language=target_language)
        llm = get_llm()

        # ä¸»å¯¹è¯
        yield {
            "type": "thinking",
            "content": localize(target_language, en="Planning...", zh="æ­£åœ¨è§„åˆ’è§£ç­”..."),
        }

        # æ£€ç´¢ç›¸å…³çŸ¥è¯†
        knowledge_context = format_knowledge_context([], language=target_language)
        kb = get_knowledge_base()
        if kb:
            try:
                stats = kb.get_stats()
                print(f"[çŸ¥è¯†åº“] çŠ¶æ€: {stats['total_entries']} æ¡çŸ¥è¯†")
                relevant_knowledge = kb.search(query=message)
                print(f"[çŸ¥è¯†åº“] æ£€ç´¢åˆ° {len(relevant_knowledge)} æ¡ç›¸å…³çŸ¥è¯†")
                if relevant_knowledge:
                    knowledge_context = format_knowledge_context(
                        relevant_knowledge,
                        language=target_language,
                    )
                    yield {
                        "type": "thinking",
                        "content": localize(
                            target_language,
                            en=f"Found {len(relevant_knowledge)} relevant knowledge items...",
                            zh=f"æ‰¾åˆ° {len(relevant_knowledge)} æ¡ç›¸å…³çŸ¥è¯†å‚è€ƒ...",
                        ),
                    }
            except Exception as e:
                # çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
                print(f"[çŸ¥è¯†åº“æ£€ç´¢] è­¦å‘Š: {e}")
                import traceback
                traceback.print_exc()
        else:
            print("[çŸ¥è¯†åº“] æœªå¯ç”¨æˆ–åˆå§‹åŒ–å¤±è´¥")
        
        system_prompt = SYSTEM_PROMPT_WITH_TOOLS.format(
            excel_summary=excel_summary,
            tools_description=TOOLS_DESCRIPTION,
            knowledge_context=knowledge_context,
            target_language=language_label(target_language),
        )
        
        # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«å†å²è®°å½•
        conversation = [SystemMessage(content=system_prompt)]
        
        # è·å–å½“å‰æ´»è·ƒè¡¨ä¿¡æ¯
        active_table_info = loader.get_active_table_info()
        current_table_name = active_table_info.filename if active_table_info else localize(
            target_language, en="Unknown table", zh="æœªçŸ¥è¡¨"
        )
        
        # æ·»åŠ å†å²å¯¹è¯ï¼ˆåŒ…å«è¡¨åæ ‡è®°ï¼‰
        if history:
            for h in history:
                content = h.get("content", "")
                table_name = h.get("tableName", "")
                
                # å¦‚æœå†å²æ¶ˆæ¯æœ‰è¡¨åï¼Œä¸”ä¸å½“å‰è¡¨ä¸åŒï¼Œæ·»åŠ æ ‡è®°
                if table_name and h.get("role") == "user":
                    tag = localize(target_language, en="For table", zh="é’ˆå¯¹è¡¨")
                    content = f"[{tag}: {table_name}] {content}"
                
                if h.get("role") == "user":
                    conversation.append(HumanMessage(content=content))
                elif h.get("role") == "assistant":
                    conversation.append(AIMessage(content=content))

        # æ·»åŠ å½“å‰æ¶ˆæ¯ï¼ˆæ ‡è®°å½“å‰è¡¨ï¼‰
        tag = localize(target_language, en="Current table", zh="å½“å‰æ“ä½œè¡¨")
        current_message = f"[{tag}: {current_table_name}] {message}"
        conversation.append(HumanMessage(content=current_message))

        # æ›´æ–° prompt - ç®€åŒ–æŒ‡ä»¤ï¼Œé¿å…è¿‡åº¦æ€è€ƒ
        conversation[0].content += """
**IMPORTANT INSTRUCTIONS:**
1. If you need to use a tool to answer the question, output the tool call JSON immediately
2. Do NOT write long explanations before calling tools
3. After getting tool results, provide a clear answer in the TARGET RESPONSE LANGUAGE
4. Be direct and action-oriented
"""
        
        max_iterations = 50
        
        for _ in range(max_iterations):
            response = await llm.ainvoke(conversation)
            response_text = response.content
            
            # è§£æå·¥å…·è°ƒç”¨
            tool_call = parse_tool_call(response_text)
            
            if tool_call and "tool" in tool_call:
                # å°è¯•æå– JSON ä¹‹å‰çš„æ€è€ƒæ–‡æœ¬
                thought_text = ""
                json_start = response_text.find('{')
                if json_match := re.search(r'```json', response_text):
                    thought_text = response_text[:json_match.start()].strip()
                elif json_start > 0:
                    thought_text = response_text[:json_start].strip()
                
                # å¦‚æœæœ‰æ€è€ƒæ–‡æœ¬ä¸”é•¿åº¦è¶³å¤Ÿï¼Œå‘é€æ›´æ–°
                if thought_text and len(thought_text) > 2:
                    yield {"type": "thinking", "content": thought_text}
                
                yield {"type": "thinking_done"}

                tool_name = tool_call["tool"]
                tool_args = tool_call.get("args", {})
                
                yield {
                    "type": "tool_call",
                    "name": tool_name,
                    "args": tool_args,
                }
                
                # æ‰§è¡Œå·¥å…·
                tool_result = execute_tool(tool_name, tool_args)
                
                yield {
                    "type": "tool_result",
                    "name": tool_name,
                    "result": tool_result,
                }
                
                # å°†å·¥å…·ç»“æœä½œä¸ºæ–°æ¶ˆæ¯ç»§ç»­å¯¹è¯
                result_message = localize(
                    target_language,
                    en=(
                        f"Tool `{tool_name}` result:\n```json\n"
                        f"{json_dumps(tool_result, ensure_ascii=False, indent=2)}\n```\n\n"
                        f"Answer the user's question using this result. Respond in {language_label(target_language)}."
                    ),
                    zh=(
                        f"å·¥å…· `{tool_name}` æ‰§è¡Œç»“æœï¼š\n```json\n"
                        f"{json_dumps(tool_result, ensure_ascii=False, indent=2)}\n```\n\n"
                        "è¯·æ ¹æ®è¿™ä¸ªç»“æœå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
                    ),
                )

                # Only persist the clean JSON tool call (avoid leaking non-JSON chatter).
                conversation.append(AIMessage(content=json_dumps(tool_call, ensure_ascii=False)))
                conversation.append(HumanMessage(content=result_message))
                
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¾“å‡ºå“åº”
                final_text = response_text
                print(f"[Language Check] Target: {target_language}, Response length: {len(final_text)}")
                mismatch = is_language_mismatch(target_language, final_text)
                print(f"[Language Check] Is mismatch: {mismatch}")

                if mismatch:
                    print(f"[Language Rewrite] Rewriting to {language_label(target_language)}")
                    rewritten = await llm.ainvoke(
                        [
                            SystemMessage(content=rewrite_system_prompt(target_language)),
                            HumanMessage(content=final_text),
                        ]
                    )
                    if isinstance(rewritten, AIMessage) and rewritten.content:
                        final_text = rewritten.content
                        print(f"[Language Rewrite] Success, new length: {len(final_text)}")

                yield {"type": "token", "content": final_text}
                yield {"type": "done", "content": final_text}
                return

        yield {
            "type": "error",
            "content": localize(target_language, en="Reached max iterations.", zh="è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°"),
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        yield {"type": "thinking_done"}
        yield {
            "type": "error",
            "content": localize(
                target_language,
                en=f"Error: {str(e)}",
                zh=f"å¤„ç†å‡ºé”™: {str(e)}",
            ),
        }
